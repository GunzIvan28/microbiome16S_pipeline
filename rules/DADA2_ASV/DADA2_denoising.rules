rule DADA2_q_filtering:
    conda:
        "../../envs/DADA2_in_R.yml"
    input:
        R1_list = "DADA2/1a_trimmed_primers/{sample}_trimmed_R1.fastq.gz",
        R2_list = "DADA2/1a_trimmed_primers/{sample}_trimmed_R2.fastq.gz",
    output:
        q_score_filtered_Fs = "DADA2/1b_q_score_filtered_paired/{sample}_filtered_R1.fastq.gz",
        q_score_filtered_Rs = "DADA2/1b_q_score_filtered_paired/{sample}_filtered_R2.fastq.gz",
        #filtering_stats = "DADA2/1b_q_score_filtered_paired/DADA2_q_score_filtering_stats.tsv"
    log:
        logging_folder + "DADA2/1b_q_score_filtered/{sample}_DADA2_1b_q_score_filtering.txt"
    params:
        F_reads_length_trim = config["DADA2_F_reads_length_trim"],
        R_reads_length_trim = config["DADA2_R_reads_length_trim"],
        F_extected_error = config["F_extected_error"],
        R_extected_error = config["R_extected_error"]
    threads:
        1
    script:
        "scripts/1_DADA2_q_filtering.R"


rule DADA2_learn_errors:
    conda:
        "../../envs/DADA2_in_R.yml"
    input:
        q_score_filtered_Fs = lambda wildcards: expand("DADA2/1b_q_score_filtered_paired/{sample}_filtered_R1.fastq.gz", sample = all_samples.index.values[all_samples.RUN==wildcards.RUN]),
        q_score_filtered_Rs = lambda wildcards: expand("DADA2/1b_q_score_filtered_paired/{sample}_filtered_R2.fastq.gz", sample = all_samples.index.values[all_samples.RUN==wildcards.RUN]),
        #q_score_filtering_stats = "DADA2/1b_q_score_filtered_paired/DADA2_q_score_filtering_stats.tsv"
    output:
        error_profile_F = "DADA2/2_denoised/{RUN}/error_profile_F.Rds",
        error_profile_R = "DADA2/2_denoised/{RUN}/error_profile_R.Rds",
        #filtering_stats = "DADA2/2_denoised/{RUN}/DADA2_denoising_stats.tsv"
    log:
        logging_folder + "DADA2/2_denoised/{RUN}/DADA2_2_error_profile.txt"
    threads:
        1
    script:
        "scripts/2a_big_data_DADA2_learn_errors.R"


rule DADA2_infer_ASV:
    conda:
        "../../envs/DADA2_in_R.yml"
    input:
        q_score_filtered_Fs = "DADA2/1b_q_score_filtered_paired/{sample}_filtered_R1.fastq.gz",
        q_score_filtered_Rs = "DADA2/1b_q_score_filtered_paired/{sample}_filtered_R2.fastq.gz",
        error_profile_F = "DADA2/2_denoised/{RUN}/error_profile_F.Rds",
        error_profile_R = "DADA2/2_denoised/{RUN}/error_profile_R.Rds",
        #q_score_filtering_stats = "DADA2/1b_q_score_filtered_paired/DADA2_q_score_filtering_stats.tsv"
    output:
        sample_seq_tab = "DADA2/2_denoised/{RUN}/{sample}_seq_tab.Rds",
        #filtering_stats = "DADA2/2_denoised/{RUN}/DADA2_denoising_stats.tsv"
    params:
        sample_name = lambda wildcards: wildcards.sample,
    log:
        logging_folder + "DADA2/2_denoised/{RUN}/{sample}_DADA2_2_infer_ASV.txt"
    threads:
        1
    script:
        "scripts/2b_big_data_DADA2_infer_ASV.R"


rule DADA2_merge_sample_ASV:
    conda:
        "../../envs/DADA2_in_R.yml"
    input:
        sample_seq_table = lambda wildcards: expand("DADA2/2_denoised/{{RUN}}/{sample}_seq_tab.Rds", sample = all_samples.index.values[all_samples.RUN==wildcards.RUN]),
        #q_score_filtering_stats = "DADA2/1b_q_score_filtered_paired/DADA2_q_score_filtering_stats.tsv"
    output:
        merged_seq_table = "DADA2/2_denoised/{RUN}/seq_tab.Rds",
        #filtering_stats = "DADA2/2_denoised/{RUN}/DADA2_denoising_stats.tsv"
    log:
        logging_folder + "DADA2/2_denoised/{RUN}/DADA2_2_merge_sample_ASV.txt"
    threads:
        1
    script:
        "scripts/2c_big_data_DADA2_merge_ASV.R"


rule DADA2_merge_filter_chim:
    conda:
        "../../envs/DADA2_in_R.yml"
    input:
        seq_tab = expand("DADA2/2_denoised/{RUN}/seq_tab.Rds", RUN=list(set(all_samples[config["run_column"]])))
    output:
        with_chim = "DADA2/2_denoised/dna-sequences_with_chim.fasta",
        no_chim = "DADA2/2_denoised/dna-sequences_no_chim.fasta",
        length_filtered = "DADA2/2_denoised/dna-sequences_long_names.fasta",
        renamed = "DADA2/2_denoised/dna-sequences.fasta",
        count_table = "DADA2/2_denoised/count_table.txt",
        #filtering_stats = "DADA2/2_denoised/DADA2_denoising_stats.tsv"
    log:
        logging_folder + "DADA2/2_denoised/DADA2_2_merge_filter_chim.txt"
    params:
        merged_min_length = config["merged_min_length"],
        merged_max_length = config["merged_max_length"]
    threads:
        1
    script:
        "scripts/2d_big_data_DADA2_merge_chimera.R"

