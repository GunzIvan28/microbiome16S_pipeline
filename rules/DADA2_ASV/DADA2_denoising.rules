rule DADA2_q_filtering:
    conda:
        "../../envs/DADA2_in_R.yml"
    input:
        R1_list = "DADA2/1a_trimmed_primers/{sample}_trimmed_R1.fastq.gz",
        R2_list = "DADA2/1a_trimmed_primers/{sample}_trimmed_R2.fastq.gz",
    output:
        q_score_filtered_F = "DADA2/1b_q_score_filtered_paired/{sample}_filtered_R1.fastq.gz",
        q_score_filtered_R = "DADA2/1b_q_score_filtered_paired/{sample}_filtered_R2.fastq.gz",
        q_filtering_stats = "DADA2/1b_q_score_filtered_paired/{sample}_q_score_filtering_stats.rds"
    log:
        logging_folder + "DADA2/1b_q_score_filtered/{sample}_DADA2_1b_q_score_filtering.txt"
    params:
        F_reads_length_trim = config["DADA2_F_reads_length_trim"],
        R_reads_length_trim = config["DADA2_R_reads_length_trim"],
        F_reads_extected_error = config["F_extected_error"],
        R_reads_extected_error = config["R_extected_error"],
        sample_name = lambda wildcards: wildcards.sample,
    threads:
        1
    script:
        "scripts/1_DADA2_q_filtering.R"


rule DADA2_learn_errors:
    conda:
        "../../envs/DADA2_in_R_detailed.yml"
    input:
        q_score_filtered_F = lambda wildcards: expand("DADA2/1b_q_score_filtered_paired/{sample}_filtered_R1.fastq.gz", sample = all_samples.index.values[all_samples[config["run_column"]] == wildcards.RUN]),
        q_score_filtered_R = lambda wildcards: expand("DADA2/1b_q_score_filtered_paired/{sample}_filtered_R2.fastq.gz", sample = all_samples.index.values[all_samples[config["run_column"]] == wildcards.RUN])
    output:
        error_profile_F = "DADA2/2_denoised/{RUN}/error_profile_F.rds",
        error_profile_R = "DADA2/2_denoised/{RUN}/error_profile_R.rds",
        error_profile_F_plot = "DADA2/2_denoised/{RUN}/error_profile_F_plot.png",
        error_profile_R_plot = "DADA2/2_denoised/{RUN}/error_profile_R_plot.png",
    log:
        logging_folder + "DADA2/2_denoised/{RUN}/DADA2_2_error_profile.txt"
    threads:
        1
    script:
        "scripts/2a_big_data_DADA2_learn_errors.R"


rule DADA2_infer_ASV:
    conda:
        "../../envs/DADA2_in_R.yml"
    input:
        q_score_filtered_F = "DADA2/1b_q_score_filtered_paired/{sample}_filtered_R1.fastq.gz",
        q_score_filtered_R = "DADA2/1b_q_score_filtered_paired/{sample}_filtered_R2.fastq.gz",
        error_profile_F = "DADA2/2_denoised/{RUN}/error_profile_F.rds",
        error_profile_R = "DADA2/2_denoised/{RUN}/error_profile_R.rds",
    output:
        infer_stats = "DADA2/2_denoised/{RUN}/{sample}_infer_stats.rds",
        sample_seq_tab = "DADA2/2_denoised/{RUN}/{sample}_infer_seq_tab.rds",
    params:
        sample_name = lambda wildcards: wildcards.sample,
        run = lambda wildcards: wildcards.RUN,
        x_axis_column_value = lambda wildcards: all_samples[config["x_axis_column"]][wildcards.sample]
    log:
        logging_folder + "DADA2/2_denoised/{RUN}/{sample}_DADA2_2_infer_ASV.txt"
    threads:
        1
    script:
        "scripts/2b_big_data_DADA2_infer_ASV.R"


rule DADA2_merge_sample_ASV:
    conda:
        "../../envs/DADA2_in_R.yml"
    input:
        infer_stats = lambda wildcards : expand("DADA2/2_denoised/{{RUN}}/{sample}_infer_stats.rds", sample = all_samples.index.values[all_samples[config["run_column"]] == wildcards.RUN]),
        sample_seq_tab = lambda wildcards: expand("DADA2/2_denoised/{{RUN}}/{sample}_infer_seq_tab.rds", sample = all_samples.index.values[all_samples[config["run_column"]] == wildcards.RUN]),
    output:
        run_stats = "DADA2/2_denoised/{RUN}/run_stats.rds",
        run_seq_table = "DADA2/2_denoised/{RUN}/run_seq_tab.rds",
    log:
        logging_folder + "DADA2/2_denoised/{RUN}/DADA2_2_merge_sample_ASV.txt"
    threads:
        1
    script:
        "scripts/2c_big_data_DADA2_merge_ASV.R"


rule DADA2_merge_filter_chim:
    conda:
        "../../envs/DADA2_in_R.yml"
    input:
        seq_tab = expand("DADA2/2_denoised/{RUN}/run_seq_tab.rds", RUN=list(set(all_samples[config["run_column"]])))
    output:
        no_chim = "DADA2/2_denoised/dna-sequences_no_chim.rds",
        length_filtered = "DADA2/2_denoised/dna-sequences_long_names.rds",
        renamed = "DADA2/2_denoised/dna-sequences.fasta",
        count_table = "DADA2/2_denoised/count_table.txt",
    log:
        logging_folder + "DADA2/2_denoised/DADA2_2_merge_filter_chim.txt"
    params:
        merged_min_length = config["merged_min_length"],
        merged_max_length = config["merged_max_length"]
    threads:
        1
    script:
        "scripts/2d_big_data_DADA2_merge_chimera.R"


rule DADA2_filter_stats:
    conda:
        "../../envs/DADA2_in_R.yml"
    input:
        q_filtering_stats = expand("DADA2/1b_q_score_filtered_paired/{sample}_q_score_filtering_stats.rds", sample = all_samples.index.values),
        run_stats = expand("DADA2/2_denoised/{RUN}/run_stats.rds", RUN=list(set(all_samples[config["run_column"]]))),
        no_chim = "DADA2/2_denoised/dna-sequences_no_chim.rds",
        length_filtered = "DADA2/2_denoised/dna-sequences_long_names.rds",
    output:
        filtering_stats = "DADA2/2_denoised/DADA2_denoising_stats.tsv",
    log:
        logging_folder + "DADA2/2_denoised/DADA2_denoising_stats.txt"
    threads:
        1
    priority:
        1
    script:
        "scripts/3_big_data_DADA2_filtering_stats.R"
