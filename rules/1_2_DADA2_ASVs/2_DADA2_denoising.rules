## Process trimmed sequences with DADA2 to correct errors and generate ASVs. Follows the "big data" protocol (https://benjjneb.github.io/dada2/bigdata.html)

## Define trimmed or not trimmed input (for read already trimmed)

def DADA2_trim_input_paired():
    if config['Trim_primers'] == True :
        return ["DADA2/1a_trimmed_primers/{sample}_trimmed_R1.fastq.gz", "DADA2/1a_trimmed_primers/{sample}_trimmed_R2.fastq.gz"]
    else : 
        return ["raw_reads/{sample}_R1.fastq.gz", "raw_reads/{sample}_R2.fastq.gz"]


def DADA2_trim_input_single():
    if config['Trim_primers'] == True :
        return ["DADA2/1a_trimmed_primers/{sample}_trimmed_single.fastq.gz"]
    else : 
        return ["raw_reads/{sample}_single.fastq.gz"]


### Remove low quality and too short reads
rule DADA2_q_filtering_paired:
    conda:
        "../../envs/DADA2_in_R.yml"
    container:
        singularity_envs["dada2"]
    input:
        DADA2_trim_input_paired()
    output:
        q_score_filtered_F = temp("DADA2/1b_q_score_filtered_paired/{sample}_filtered_R1.fastq.gz"),
        q_score_filtered_R = temp("DADA2/1b_q_score_filtered_paired/{sample}_filtered_R2.fastq.gz"),
        q_filtering_stats = "DADA2/1b_q_score_filtered_paired/{sample}_q_score_filtering_stats.rds"
    log:
        logging_folder + "DADA2/1b_q_score_filtered/{sample}_DADA2_1b_q_score_filtering.txt"
    params:
        F_reads_length_trim = config["DADA2_F_reads_length_trim"],
        R_reads_length_trim = config["DADA2_R_reads_length_trim"],
        F_reads_extected_error = config["F_extected_error"],
        R_reads_extected_error = config["R_extected_error"],
        sample_name = lambda wildcards: wildcards.sample,
    threads:
        1
    script:
        "scripts/1_DADA2_q_filtering_paired.R"


### Remove low quality and too short reads
rule DADA2_q_filtering_single:
    conda:
        "../../envs/DADA2_in_R.yml"
    container:
        singularity_envs["dada2"]
    input:
        DADA2_trim_input_single()
    output:
        q_score_filtered_F = temp("DADA2/1b_q_score_filtered_paired/{sample}_filtered_single.fastq.gz"),
        q_filtering_stats = "DADA2/1b_q_score_filtered_paired/{sample}_q_score_filtering_stats.rds"
    log:
        logging_folder + "DADA2/1b_q_score_filtered/{sample}_DADA2_1b_q_score_filtering.txt"
    params:
        F_reads_length_trim = config["DADA2_F_reads_length_trim"],
        F_reads_extected_error = config["F_extected_error"],
        sample_name = lambda wildcards: wildcards.sample,
    threads:
        1
    script:
        "scripts/1_DADA2_q_filtering_single.R"



### Learn error profile of the runs
rule DADA2_learn_errors_paired:
    conda:
        "../../envs/DADA2_in_R.yml"
    container:
        singularity_envs["dada2"]
    input:
        q_score_filtered_F = lambda wildcards: expand("DADA2/1b_q_score_filtered_paired/{sample}_filtered_R1.fastq.gz", sample = all_samples.index.values[all_samples[config["run_column"]] == wildcards.RUN]),
        q_score_filtered_R = lambda wildcards: expand("DADA2/1b_q_score_filtered_paired/{sample}_filtered_R2.fastq.gz", sample = all_samples.index.values[all_samples[config["run_column"]] == wildcards.RUN])
    output:
        error_profile_F = "DADA2/2_denoised/{RUN}/error_profile_F.rds",
        error_profile_R = "DADA2/2_denoised/{RUN}/error_profile_R.rds",
        error_profile_F_plot = report("DADA2/2_denoised/{RUN}/error_profile_F_plot.png", caption="report/DADA2_error_profile.rst", category="DADA2", subcategory="Error profile"),
        error_profile_R_plot = report("DADA2/2_denoised/{RUN}/error_profile_R_plot.png", caption="report/DADA2_error_profile.rst", category="DADA2", subcategory="Error profile"),
    log:
        logging_folder + "DADA2/2_denoised/{RUN}/DADA2_learn_errors.txt"
    threads:
        8
    script:
        "scripts/2a_big_data_DADA2_learn_errors.R"



### Learn error profile of the runs
rule DADA2_learn_errors_single:
    conda:
        "../../envs/DADA2_in_R.yml"
    container:
        singularity_envs["dada2"]
    input:
        q_score_filtered_F = lambda wildcards: expand("DADA2/1b_q_score_filtered_paired/{sample}_filtered_single.fastq.gz", sample = all_samples.index.values[all_samples[config["run_column"]] == wildcards.RUN]),
    output:
        error_profile_F = "DADA2/2_denoised/{RUN}/error_profile_F.rds",
        error_profile_R = "DADA2/2_denoised/{RUN}/error_profile_R.rds",
        error_profile_F_plot = report("DADA2/2_denoised/{RUN}/error_profile_F_plot.png", caption="report/DADA2_error_profile.rst", category="DADA2", subcategory="Error profile"),
        error_profile_R_plot = report("DADA2/2_denoised/{RUN}/error_profile_R_plot.png", caption="report/DADA2_error_profile.rst", category="DADA2", subcategory="Error profile"),
    log:
        logging_folder + "DADA2/2_denoised/{RUN}/DADA2_learn_errors.txt"
    threads:
        8
    script:
        "scripts/2a_big_data_DADA2_learn_errors.R"

        

### Infer error free sequences from the filtered reads and the runs error profiles
def sample_list_run_QC(RUN):
    file_list = []
    samples =  list(all_samples.index.values[all_samples[config["run_column"]] == RUN])
    for s in samples:
        suffix_s = reads_ext[s]
        combined_values = expand("QC/FastQC/raw_reads/{RUN}/{sample}_{suffix}_fastqc.zip", RUN = RUN, sample = s, suffix = suffix_s)
        file_list = file_list + combined_values

    return(file_list)



rule DADA2_infer_paired_ASV:
    conda:
        "../../envs/DADA2_in_R.yml"
    container:
        singularity_envs["dada2"]
    input:
        q_score_filtered_F = "DADA2/1b_q_score_filtered_paired/{sample}_filtered_R1.fastq.gz",
        q_score_filtered_R = "DADA2/1b_q_score_filtered_paired/{sample}_filtered_R2.fastq.gz",
        error_profile_F = "DADA2/2_denoised/{RUN}/error_profile_F.rds",
        error_profile_R = "DADA2/2_denoised/{RUN}/error_profile_R.rds",
    output:
        infer_stats = "DADA2/2_denoised/{RUN}/{sample}_infer_stats.rds",
        sample_seq_tab = "DADA2/2_denoised/{RUN}/{sample}_infer_seq_tab.rds",
    params:
        sample_name = lambda wildcards: wildcards.sample,
        run = lambda wildcards: wildcards.RUN,
        x_column_value = lambda wildcards: all_samples[config["sample_label"]][wildcards.sample],
	min_overlap =  config["min_overlap"]
    log:
        logging_folder + "DADA2/2_denoised/{RUN}/{sample}_DADA2_2_infer_ASV.txt"
    threads:
        4
    script:
        "scripts/2b_big_data_DADA2_infer_ASV.R"



rule DADA2_infer_single_ASV:
    conda:
        "../../envs/DADA2_in_R.yml"
    container:
        singularity_envs["dada2"]
    input:
        q_score_filtered_F = "DADA2/1b_q_score_filtered_paired/{sample}_filtered_single.fastq.gz",
        error_profile_F = "DADA2/2_denoised/{RUN}/error_profile_single.rds",
    output:
        infer_stats = "DADA2/2_denoised/{RUN}/{sample}_infer_stats.rds",
        sample_seq_tab = "DADA2/2_denoised/{RUN}/{sample}_infer_seq_tab.rds",
    params:
        sample_name = lambda wildcards: wildcards.sample,
        run = lambda wildcards: wildcards.RUN,
        x_column_value = lambda wildcards: all_samples[config["sample_label"]][wildcards.sample],
	min_overlap =  config["min_overlap"]
    log:
        logging_folder + "DADA2/2_denoised/{RUN}/{sample}_DADA2_2_infer_ASV.txt"
    threads:
        4
    script:
        "scripts/2b_big_data_DADA2_infer_ASV.R"





### Merge the ASVs determined from the different samples
rule DADA2_merge_sample_ASV:
    conda:
        "../../envs/DADA2_in_R.yml"
    container:
        singularity_envs["dada2"]
    input:
        infer_stats = lambda wildcards : expand("DADA2/2_denoised/{{RUN}}/{sample}_infer_stats.rds", sample = all_samples.index.values[all_samples[config["run_column"]] == wildcards.RUN]),
        sample_seq_tab = lambda wildcards: expand("DADA2/2_denoised/{{RUN}}/{sample}_infer_seq_tab.rds", sample = all_samples.index.values[all_samples[config["run_column"]] == wildcards.RUN]),
    output:
        run_stats = "DADA2/2_denoised/{RUN}/run_stats.rds",
        run_seq_table = "DADA2/2_denoised/{RUN}/run_seq_tab.rds",
    log:
        logging_folder + "DADA2/2_denoised/{RUN}/DADA2_2_merge_sample_ASV.txt"
    threads:
        1
    script:
        "scripts/2c_big_data_DADA2_merge_ASV.R"


### Merge data from all run, filter out chimera and remove too short merged sequences
rule DADA2_merge_runs_filter_chim:
    conda:
        "../../envs/DADA2_in_R.yml"
    container:
        singularity_envs["dada2"]
    input:
        seq_tab = expand("DADA2/2_denoised/{RUN}/run_seq_tab.rds", RUN=list(set(all_samples[config["run_column"]])))
    output:
        no_chim = "DADA2/2_denoised/dna-sequences_no_chim.rds",
        length_filtered = "DADA2/2_denoised/dna-sequences_long_names.rds",
        rep_seqs = report("DADA2/2_denoised/dna-sequences.fasta",caption="report/DADA2_output.rst", category="DADA2", subcategory="Output"),
        count_table = report("DADA2/2_denoised/count_table.tsv",caption="report/DADA2_output.rst", category="DADA2", subcategory="Output"),
        length_histo = report("DADA2/2_denoised/merged_reads_length.png", caption="report/DADA2_output.rst", category="DADA2", subcategory="Output")
    log:
        logging_folder + "DADA2/2_denoised/DADA2_2_merge_filter_chim.txt"
    params:
        merged_min_length = config["merged_min_length"],
        merged_max_length = config["merged_max_length"]
    threads:
        4
    script:
        "scripts/2d_big_data_DADA2_merge_chimera.R"

### Generate filtration stats
rule DADA2_filter_stats:
    conda:
        "../../envs/amplicons_r_utils.yml"
    container:
        singularity_envs["r_utils"]
    input:
        q_filtering_stats = expand("DADA2/1b_q_score_filtered_paired/{sample}_q_score_filtering_stats.rds", sample = all_samples.index.values),
        run_stats = expand("DADA2/2_denoised/{RUN}/run_stats.rds", RUN=list(set(all_samples[config["run_column"]]))),
        no_chim = "DADA2/2_denoised/dna-sequences_no_chim.rds",
        length_filtered = "DADA2/2_denoised/dna-sequences_long_names.rds",
    output:
        filtering_stats = report("DADA2/2_denoised/DADA2_denoising_stats.tsv", caption="report/DADA2_output.rst", category="DADA2", subcategory="Output")
    log:
        logging_folder + "DADA2/2_denoised/DADA2_denoising_stats.txt"
    threads:
        1
    priority:
        1
    script:
        "scripts/2e_big_data_DADA2_filtering_stats.R"


### Accessory rules to generate QC reports after q-score filtering. Not in regular use
rule assess_quality_paired_filtered_reads_with_fastqc :
    conda:
        "../../envs/FastQC.yml"
    container:
        singularity_envs["fastqc"]
    input:
        "DADA2/1b_q_score_filtered_paired/{sample}_filtered_R1.fastq.gz",
        "DADA2/1b_q_score_filtered_paired/{sample}_filtered_R2.fastq.gz",
    output:
        "QC/FastQC/DADA2_filtered/{sample}_filtered_R1_fastqc.zip",
        "QC/FastQC/DADA2_filtered/{sample}_filtered_R2_fastqc.zip",
    log:
        logging_folder + "QC/FastQC/DADA2_filtered/{sample}_filtered_R1_fastqc.txt"
    shell:
        """
        fastqc {input} -o $(dirname {output[0]}) &> {log}
        """

rule create_filtered_reads_multiqc_report:
    conda:
        "../../envs/MultiQC.yml"
    container:
        singularity_envs["multiqc"]
    input:
       expand("QC/FastQC/DADA2_filtered/{sample}_filtered_R1_fastqc.zip", sample = list(read_naming.keys())),
       expand("QC/FastQC/DADA2_filtered/{sample}_filtered_R2_fastqc.zip", sample = list(read_naming.keys())),
       expand(logging_folder + "DADA2/1a_trimmed_primers/{sample}_trimmed.txt", sample = list(read_naming.keys()))
    output:
        report("QC/multiqc_DADA2_filtered_reads_report.html", caption="report/DADA2_output.rst", category="DADA2", subcategory="MultiQC")
    log:
        logging_folder + "QC/multiqc_filtered_reads_report.txt"
#    params:
#        configfile = multiqc_configfile
    shell:
        """
        multiqc -f -n {output[0]} $(dirname {input} | tr "\n" " ") --cl_config "max_table_rows: 100000" &> {log}
        """
