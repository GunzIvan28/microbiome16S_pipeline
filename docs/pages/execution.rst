
Pipeline execution
==================

Upon execution, the snakemake require to associate the demanded output with the input sequencing read files. RSP4ABM is designed to accept three scenarios regarding sequencing reads locations: 

1. Name-based matching of reads
Requirement: 
- all reads are located in one single directory (named *links* by default).
- all reads have a simp

Working principle: 

Choose this option:

2. Name-based matching of the reads, with reads renaming (OldSampleName)


2. Local path designation of reads

3. Sequence Read Archive (SRA) deposited reads

 




*Provided command-line examples are for a standard unix terminal in bash* 

.............

0. Working directory
---------------------------------

Before the execution of the pipeline, prepare a new dedicated directory somewhere on your computer where you have sufficient space (and NOT within the pipeline folder).

*for instance*::

    #make a new directory named "new_analysis"
    $ mkdir new_analysis

    # Move to the new directory
    $ cd new_analysis

Then, your working directory must contain the following elements: 
    - config file
    - sample sheet
    - a link directory (optional)

1. Config file:
---------------------------------

Parameters must be provided to adapt the pipeline to the specificities of your analysis. This is done through a config file in the "*.yaml*" format. Create this file in your working directory, copy the content of the example below and adapt it to your analysis. The three first parameters (*"link_directory", *"sra_samples"* and *"local_samples"*) are associated to the definition of input files and are explained hereunder. For the meaning of the other parameters, please refer to the short comment next by each parameter and the detailed description of the pipeline on the :ref:`under_the_hood` page.

*for instance*::

    # Open a graphic text editor and create a config file. Once opened, copy the example below and adapt it. 
    $ gedit config.yaml

    # Or use a command-line text editor, e.g. 
    # $ nano config.yaml 


**Config file example:**

.. literalinclude:: ../../ressources/template_files/config.yaml
    :language: yaml


2. Sample sheet (metadata):
---------------------------------
The pipeline requires a spreadsheet in `tabulation-separated values (tsv) format <https://en.wikipedia.org/wiki/Tab-separated_values>`_ listing all the file in your analysis and where:

- A leftmost "*Sample*" column is an unique sample identifier. 
    *This identifier must be unique, start with a letter and not a number and cannot contain spaces or "-". "_" are OK*  
    
- a "*run_column*" describes the sequencing run of each sample.
    *Each sample must have a different value under this column for each sequencing run included in the analysis. If all samples were sequenced together, then the same value must be repeated for all samples. Prefer an alphanumeric factor, e.g. "run_20200101"*

- a "*grouping_column*" regroups the samples for visualization purpose. 
    *Some of the visualization generated by the pipeline will be generated individually for each value contained in the "grouping column"*

- a "*sample_label*" describes each sample.
    *This column must provide a unique, explicit description of each sample. It can be a replication of the *Sample* column but also provides the opportunity to have more concise or explicit description of each sample*.

- optional (but recommended) columns describes technical metadata. 
    *In this recommended to provide technical metadata (e.g. library preparation DNA yield) to support technical QC of the data*

- optional (but recommended) columns describes experimental or clinical metadata.
    *In this recommended to provide clinical or experimental description of each sample, which will support later interpretation of the data.*


**Sample sheet file example:**

.. literalinclude:: ../../ressources/template_files/example_local_samples.tsv 
    :language: tsv
    :start-after: #Example:



Method 1: local *.fastq* matching 
---------------------------------
With this method, you will deposit all required in a "*links*" folder. The files in this folder can be the actual files or symbolic links to you reads. The pipeline with 
